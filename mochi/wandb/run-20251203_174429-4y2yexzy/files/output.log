[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
--- å…¨ä½“å®Ÿè¡Œç”¨wandb runã‚’é–‹å§‹ã—ã¾ã—ãŸ (run: overall_run_20251203_174426) ---
WARNING:spreadsheet_integration:ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“
WARNING:spreadsheet_integration:ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®šãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚é€£æºæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚
è­¦å‘Š: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºã®è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é€£æºã¯ç„¡åŠ¹ã§ã™ã€‚

--- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ« (qwen2.5-7b-instruct) ã‚’åˆæœŸåŒ–ã—ã¾ã™ï¼ˆå…¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å†åˆ©ç”¨ï¼‰ ---
--- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ« (qwen2.5-7b-instruct) ã®åˆæœŸåŒ–ã‚’é–‹å§‹ ---
ãƒ¢ãƒ‡ãƒ« qwen2.5-7b-instruct ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™
ãƒ¢ãƒ‡ãƒ« qwen2.5-7b-instruct ã‚’åˆæœŸåŒ–ä¸­...
INFO:model_manager:ãƒ¢ãƒ‡ãƒ« qwen2.5-7b-instruct (Qwen/Qwen2.5-7B-Instruct) ã‚’åˆæœŸåŒ–ä¸­...
INFO:model_manager:ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ï¼‰: /export/ssd/fujita-h/huggingface/hub/qwen2.5-7b-instruct
INFO:model_manager:4bité‡å­åŒ–ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸ
INFO:model_manager:ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸ
`torch_dtype` is deprecated! Use `dtype` instead!
Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/yanai-lab/fujita-h/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/linecache.py", line 46, in getlines
    return updatecache(filename, module_globals)
  File "/home/yanai-lab/fujita-h/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/linecache.py", line 136, in updatecache
    with tokenize.open(fullname) as fp:
  File "/home/yanai-lab/fujita-h/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/tokenize.py", line 396, in open
    encoding, lines = detect_encoding(buffer.readline)
  File "/home/yanai-lab/fujita-h/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/tokenize.py", line 365, in detect_encoding
    first = read_or_stop()
  File "/home/yanai-lab/fujita-h/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/tokenize.py", line 323, in read_or_stop
    return readline()
KeyboardInterrupt

Original exception was:
Traceback (most recent call last):
  File "/host/home/yanai-lab/Sotsuken24/fujita-h/penguin-paper/mochi/main.py", line 1879, in <module>
    run_interviews(
  File "/host/home/yanai-lab/Sotsuken24/fujita-h/penguin-paper/mochi/main.py", line 1587, in run_interviews
    shared_local_model, shared_local_tokenizer = initialize_local_model(interviewer_model_name)
  File "/host/home/yanai-lab/Sotsuken24/fujita-h/penguin-paper/mochi/main.py", line 500, in initialize_local_model
    model, tokenizer = model_manager.initialize_model(model_key)
  File "/host/home/yanai-lab/Sotsuken24/fujita-h/penguin-paper/mochi/model_manager.py", line 331, in initialize_model
    model = AutoModelForCausalLM.from_pretrained(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4881, in from_pretrained
    hf_quantizer, config, dtype, device_map = get_hf_quantizer(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/quantizers/auto.py", line 319, in get_hf_quantizer
    hf_quantizer.validate_environment(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 103, in validate_environment
    from ..integrations import validate_bnb_backend_availability
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2317, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 2345, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/yanai-lab/fujita-h/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py", line 20, in <module>
    import bitsandbytes as bnb
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/bitsandbytes/__init__.py", line 19, in <module>
    from .backends.default import ops as default_ops
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/bitsandbytes/backends/default/ops.py", line 325, in <module>
    def _optimizer_precondition_32bit(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/__init__.py", line 2628, in compile
    return torch._dynamo.optimize(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 1052, in optimize
    return _optimize(rebuild_ctx, *args, **kwargs)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 1135, in _optimize
    backend.get_compiler_config()
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/__init__.py", line 2383, in get_compiler_config
    from torch._inductor.compile_fx import get_patched_config_dict
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py", line 24, in <module>
    import torch._inductor.async_compile  # noqa: F401 required to warm up AsyncCompile pools
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 534, in <module>
    AsyncCompile.warm_pool()
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 279, in warm_pool
    cls.process_pool()
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/async_compile.py", line 250, in process_pool
    pool = SubprocPool(get_compile_threads())
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/compile_worker/subproc_pool.py", line 124, in __init__
    torch_key_str = base64.b64encode(torch_key()).decode("utf-8")
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 718, in wrapper
    _cache.append(func())
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 760, in torch_key
    return get_code_hash(_TORCH_PATH)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/torch/_inductor/codecache.py", line 753, in get_code_hash
    build_code_hash([root], "", hasher)
