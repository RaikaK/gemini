[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
--- å…¨ä½“å®Ÿè¡Œç”¨wandb runã‚’é–‹å§‹ã—ã¾ã—ãŸ (run: llama3_overall_20sims_20251210_005642, model: llama3) ---
WARNING:spreadsheet_integration:ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“
WARNING:spreadsheet_integration:ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®šãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚é€£æºæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚
è­¦å‘Š: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºã®è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚é€£æºã¯ç„¡åŠ¹ã§ã™ã€‚

--- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ« (llama3) ã‚’åˆæœŸåŒ–ã—ã¾ã™ï¼ˆå…¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å†åˆ©ç”¨ï¼‰ ---
--- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ« (llama3) ã®åˆæœŸåŒ–ã‚’é–‹å§‹ ---
ãƒ¢ãƒ‡ãƒ« llama3 ã¯æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™
ãƒ¢ãƒ‡ãƒ« llama3 ã‚’åˆæœŸåŒ–ä¸­...
INFO:model_manager:ãƒ¢ãƒ‡ãƒ« llama3 (meta-llama/Llama-3.1-8B-Instruct) ã‚’åˆæœŸåŒ–ä¸­...
WARNING:model_manager:ãƒ¢ãƒ‡ãƒ« llama3 ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒä¸å®Œå…¨ã§ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã‚’è©¦ã¿ã¾ã™ï¼ˆæ¬ ã‘ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ï¼‰ã€‚
INFO:model_manager:4bité‡å­åŒ–ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸ
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
ERROR:model_manager:ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: not a string
ERROR:model_manager:Traceback (most recent call last):
  File "/host/home/yanai-lab/Sotsuken24/fujita-h/penguin-paper/mochi/model_manager.py", line 333, in initialize_model
    tokenizer = AutoTokenizer.from_pretrained(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1159, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2097, in from_pretrained
    return cls._from_pretrained(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2135, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2343, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py", line 171, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop("from_slow", False))
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama.py", line 198, in get_spm_processor
    tokenizer.Load(self.vocab_file)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/sentencepiece/__init__.py", line 961, in Load
    return self.LoadFromFile(model_file)
  File "/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/sentencepiece/__init__.py", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
TypeError: not a string
--- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•— ---
ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚
