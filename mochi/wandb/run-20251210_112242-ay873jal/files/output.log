[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
--- 全体実行用wandb runを開始しました (run: llama3-elyza-jp_overall_20sims_20251210_112241, model: llama3-elyza-jp) ---
WARNING:spreadsheet_integration:スプレッドシート設定ファイルが見つかりません
WARNING:spreadsheet_integration:スプレッドシート設定が読み込めません。連携機能は無効です。
警告: スプレッドシート連携の設定が見つかりません。連携は無効です。

--- ローカルモデル (llama3-elyza-jp) を初期化します（全シミュレーションで再利用） ---
--- ローカルモデル (llama3-elyza-jp) の初期化を開始 ---
モデル llama3-elyza-jp が初回選択されました。ダウンロードを開始します...
モデルサイズ: 16GB
推奨GPU: RTX 4090, A100
ダウンロードには時間がかかる場合があります。しばらくお待ちください...
INFO:model_manager:モデル llama3-elyza-jp (elyza/Llama-3-ELYZA-JP-8B) をダウンロード中...
INFO:model_manager:サイズ: 16GB
[ダウンロード進捗] ダウンロード開始: llama3-elyza-jp (16GB)
Fetching 16 files:   0%|                                                                                | 0/16 [00:00<?, ?it/s]/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/host/data/space9/fujita-h/penguin-paper/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.
For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.
  warnings.warn(
USE_POLICY.md: 4.70kB [00:00, 5.67MB/s]
Notice: 100%|██████████████████████████████████████████████████████████████████████████████████| 122/122 [00:00<00:00, 210kB/s]
config.json: 100%|████████████████████████████████████████████████████████████████████████████| 718/718 [00:00<00:00, 2.31MB/s]
generation_config.json: 100%|██████████████████████████████████████████████████████████████████| 194/194 [00:00<00:00, 301kB/s]
LICENSE: 7.80kB [00:00, 7.85MB/s]                                                                    | 0.00/194 [00:00<?, ?B/s]
README.md: 2.73kB [00:00, 2.31MB/s]
key_visual.png: 100%|███████████████████████████████████████████████████████████████████████| 425k/425k [00:00<00:00, 40.6MB/s]
.gitattributes: 1.52kB [00:00, 232kB/s]
model.safetensors.index.json: 23.9kB [00:00, 41.5MB/s]                                          | 1/16 [00:01<00:20,  1.37s/it]
special_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████| 296/296 [00:00<00:00, 596kB/s]
tokenizer_config.json: 51.0kB [00:00, 45.7MB/s]                                                    | 0.00/5.00G [00:00<?, ?B/s]
tokenizer.json: 9.09MB [00:00, 20.5MB/s]                                                           | 0.00/4.92G [00:00<?, ?B/s]
model-00004-of-00004.safetensors: 100%|███████████████████████████████████████████████████| 1.17G/1.17G [01:04<00:00, 18.0MB/s]
model-00002-of-00004.safetensors: 100%|███████████████████████████████████████████████████| 5.00G/5.00G [02:59<00:00, 27.9MB/s]
model-00001-of-00004.safetensors: 100%|███████████████████████████████████████████████████| 4.98G/4.98G [02:59<00:00, 27.8MB/s]
model-00003-of-00004.safetensors: 100%|███████████████████████████████████████████████████| 4.92G/4.92G [03:09<00:00, 25.9MB/s]
Fetching 16 files: 100%|███████████████████████████████████████████████████████████████████████| 16/16 [03:11<00:00, 11.94s/it]
INFO:model_manager:モデル llama3-elyza-jp のダウンロードが完了しました
INFO:model_manager:モデル llama3-elyza-jp (elyza/Llama-3-ELYZA-JP-8B) を初期化中...██████████████▌ | 4.78G/4.92G [03:09<00:01, 106MB/s]
[ダウンロード進捗] ダウンロード完了: llama3-elyza-jp
モデル llama3-elyza-jp のダウンロードが完了しました
モデル llama3-elyza-jp を初期化中...
INFO:model_manager:ローカルパスからロード（完全）: /export/ssd/fujita-h/huggingface/hub/llama3-elyza-jp
INFO:model_manager:4bit量子化を有効にしました
INFO:model_manager:トークナイザーのロードが完了しました
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 4/4 [02:18<00:00, 34.59s/it]
INFO:model_manager:モデルのロードが完了しました
INFO:model_manager:モデル llama3-elyza-jp の初期化が完了しました
--- ローカルモデルの初期化完了 ---
--- ローカルモデルの初期化完了 ---


================================================================================
シミュレーション 1/20 を実行中...
================================================================================

============================================================
面接ロールプレイ実行システム - シミュレーション 1
============================================================

=== セット 86 を選択 ===
企業: セラミックス技術株式会社
学生数: 3人
--- ローカルモデル (llama3-elyza-jp) を再利用 ---
--- 面接官タイプ: ローカルモデル (llama3-elyza-jp) ---

候補者: 学生VV3 (準備レベル: low, 知識カバレッジ: 5/10 (50%))

候補者: 学生VV2 (準備レベル: medium, 知識カバレッジ: 7/10 (70%))

候補者: 学生VV1 (準備レベル: high, 知識カバレッジ: 10/10 (100%))

============================================================
面接開始（20ラウンド: 全体質問と個別質問を交互に実施）
============================================================
[34m[1mwandb[0m: Finishing previous runs because reinit is set to True.
