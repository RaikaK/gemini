{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d75fcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6853a6b313beb4e81e2cd4ca', 'name': 'Yuk34', 'fullname': 'Yuki Suzumura', 'email': 's2530074@gl.cc.uec.ac.jp', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/4a3223d290ed43e4f823c4a73ede09aa.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'dx2', 'role': 'write', 'createdAt': '2025-06-19T05:58:04.286Z'}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")  # Hugging Face „ÅÆ„Ç¢„ÇØ„Çª„Çπ„Éà„Éº„ÇØ„É≥„Çí„Åì„Åì„Å´\n",
    "\n",
    "info = whoami()\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ffc703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/dx2_data/s2530074/huggingface\n",
      "/mnt/dx2_data/s2530074/huggingface/hub\n",
      "/mnt/dx2_data/s2530074/huggingface/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/dx2_data/s2530074/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/assets\"\n",
    "\n",
    "print(os.environ.get(\"HF_HOME\"))\n",
    "print(os.environ.get(\"HF_HUB_CACHE\"))\n",
    "print(os.environ.get(\"HF_ASSETS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d17e235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 21 14:37:56 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:4F:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             15W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:52:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             16W /  140W |     181MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  |   00000000:53:00.0 Off |                  Off |\n",
      "| 41%   28C    P8             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000               On  |   00000000:56:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             19W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A4000               On  |   00000000:57:00.0 Off |                  Off |\n",
      "| 41%   30C    P5             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A4000               On  |   00000000:CE:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A4000               On  |   00000000:D1:00.0 Off |                  Off |\n",
      "| 41%   31C    P5             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A4000               On  |   00000000:D2:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   8  NVIDIA RTX A4000               On  |   00000000:D5:00.0 Off |                  Off |\n",
      "| 41%   30C    P5             15W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   9  NVIDIA RTX A4000               On  |   00000000:D6:00.0 Off |                  Off |\n",
      "| 41%   32C    P3             22W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A          516481      C   ...ata/s2530074/.venv/bin/python        158MiB |\n",
      "|    2   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    3   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    4   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    5   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    6   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    7   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    8   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    9   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect() \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc5b472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7633d2aec0494f7fb5f1bdaf4b610c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f4005a9e0942fc97b0ef6ed6519770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e894db01b9477f82c35a72d9bf70dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26531c8835044b91bae7fa7aa09de851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b22571bfd44677b8a5cd6ba4ba17c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e8efefe8c84a039f9a6b30448a3d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8259de1a133f4e88874ac3117e3248d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cb52d9b89f4fc2ae4ff90bf8a92629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b12a8f6c076472fb20692ec9834c68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1208a14ea44027a0194ac9e38cf38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc142d1ad7e1496680cff57c6ae71af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6335e51a744944de9eb5dd78987d9e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a story about a dragon. A dragon that lived in a world where magic was a part of everyday life. In this world, magic was not just something that wizards and witches used, but something that was woven into the very fabric of the world. The trees were enchanted, the rivers were enchanted, and even the rocks were enchanted. And in this world, there was a dragon named Ember.\n",
      "Ember was a magnificent creature, with scales that shone like polished copper and eyes that burned like hot coals. She was a fierce and powerful dragon, with a roar that could shake the earth and wings that could carry her across the skies in a single bound. But despite her fearsome appearance, Ember was a gentle soul, with a heart full of kindness and a deep love for the world around her.\n",
      "Ember lived in a valley surrounded by towering mountains, where the air was crisp and clean and the water was sweet and pure. She spent her days basking in the warm sun, watching the clouds drift lazily by, and listening to the songs of the birds. She was a solitary creature, content to spend her time alone, but she was not antisocial. She loved to interact with the other creatures of the valley, and she was particularly fond of the ancient trees that stood sentinel around her\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. „É¢„Éá„É´Âêç\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 2. „É¢„Éá„É´„ÅÆ‰ΩúÊàê„ÄÇÁ∞°Âçò„ÅÆ„Åü„ÇÅ„Å´pipeline„Å®„ÅÑ„ÅÜ„ÉÑ„Éº„É´„ÇíÁî®„ÅÑ„Å¶„ÅÑ„Çã„ÄÇ\n",
    "# 2.1 torch_dtype=torch.float16„ÅØ16bit„ÅßË™≠„ÅøËæº„Åø„ÄÇÂæåËø∞„ÄÇ\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# 3. „Éó„É≠„É≥„Éó„ÉàÔºàÂÖ•ÂäõÊñáÔºâÔºöÂü∫Êú¨ÁöÑ„Å´„Åì„Åì„ÇíÊõ∏„ÅçÊèõ„Åà„Çã„Åì„Å®„Åå„É°„Ç§„É≥„Å´„Å™„Çã„ÄÇ\n",
    "prompt = \"Tell me a story about a dragon.\"\n",
    "\n",
    "# 4. „Åì„Åì„Åß„Éó„É≠„É≥„Éó„Éà„ÇíLLM„Å´ÂÖ•Âäõ„Åó„Å¶„ÄÅÂá∫Âäõ„ÇíÂæó„Çã„ÄÇ\n",
    "output = pipe(prompt, max_length=100, do_sample=False)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70513d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37dcbd4461c4c9d9bfd536f1c74359e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a story about a dragon.\n",
      "\n",
      "Sure! Here is a story about a dragon:\n",
      "\n",
      "Once upon a time, in a far-off land, there lived a magnificent dragon named Scorch. Scorch was the largest and most powerful dragon in all the land, with scales as red as fire and wings that stretched as wide as the sky. He lived in a great, sprawling cave deep within a mountain, surrounded by piles of\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 1. „É¢„Éá„É´Âêç\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Å®„É¢„Éá„É´„ÅÆ„É≠„Éº„Éâ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).eval()\n",
    "\n",
    "# 3. „Éó„É≠„É≥„Éó„ÉàÔºàÂÖ•ÂäõÊñáÔºâ\n",
    "prompt = \"Tell me a story about a dragon.\"\n",
    "\n",
    "# 4. „Éà„Éº„ÇØ„Éä„Ç§„Ç∫Âá¶ÁêÜ\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "## ÂÆüÈöõ„Å´„ÅØ‰ª•‰∏ã„ÅÆÂá¶ÁêÜ„Çí‰∏ÄÊ∞ó„Å´Ë°å„Å£„Å¶„ÅÑ„Çã„ÄÇ\n",
    "## token = tokenizer.tokenize(prompt)\n",
    "## input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "## input_ids_tensor = torch.tensor([input_ids])\n",
    "## attention_mask = torch.ones_like(input_ids_tensor)\n",
    "##\n",
    "## inputs = {\n",
    "##     \"input_ids\": input_ids_tensor,\n",
    "##     \"attention_mask\": attention_mask,\n",
    "## }\n",
    "\n",
    "# 5. ÁîüÊàê\n",
    "output_ids = model.generate(\n",
    "  **inputs,\n",
    "  max_length=100,\n",
    "  do_sample=False,\n",
    ")\n",
    "\n",
    "# 6. „Éà„Éº„ÇØ„É≥Âàó„Çí„ÉÜ„Ç≠„Çπ„Éà„Å´Â§âÊèõ\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "## tokens = tokenizer.convert_ids_to_tokens(output_ids[0])\n",
    "## text = tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153046ed",
   "metadata": {},
   "source": [
    "# 1.1Âá∫Âäõ„ÇíÂ§öÊßò„Å´„Åô„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e93e5be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e16cb6917c43dba2d1d4c6b63e5cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': 'Tell me a story about a dragon.\\n\\nDragons are mythical creatures that are often depicted as powerful, fire-breathing beings with wings. They are typically associated with magic, strength, and good luck. In this story, I will describe a dragon named Scorch who lives in a mystical forest filled with towering trees, sparkling streams, and a variety of magical creatures.\\n\\nScorch is a magnificent dragon'}\n",
      "{'generated_text': 'Tell me a story about a dragon.\\n\\nDragons are legendary creatures that are often depicted as powerful, fire-breathing beings with scales, wings, and a long tail. They are typically associated with magic, power, and good luck, and are often featured in mythology, folklore, and popular culture.\\n\\nHere is a story about a dragon:\\n\\nOnce upon a time, in a far-off land, there was'}\n",
      "{'generated_text': \"Tell me a story about a dragon. I want to hear about its hoard of treasure.\\n\\nOnce upon a time, in a far-off land, there lived a magnificent dragon named Scorch. Scorch was unlike any other dragon in the land, for he was the guardian of a vast and glittering hoard of treasure.\\n\\nScorch's hoard was hidden deep within a mountain cave, protected by powerful spells\"}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 1. „É¢„Éá„É´Âêç\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. „É¢„Éá„É´„ÅÆ‰ΩúÊàê„ÄÇÁ∞°Âçò„ÅÆ„Åü„ÇÅ„Å´pipeline„Å®„ÅÑ„ÅÜ„ÉÑ„Éº„É´„ÇíÁî®„ÅÑ„Å¶„ÅÑ„Çã„ÄÇ\n",
    "# 2.1 torch_dtype=torch.float16„ÅØ16bit„ÅßË™≠„ÅøËæº„Åø„ÄÇÂæåËø∞„ÄÇ\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16, num_return_sequences=3)\n",
    "\n",
    "# 3. „Éó„É≠„É≥„Éó„ÉàÔºàÂÖ•ÂäõÊñáÔºâÔºöÂü∫Êú¨ÁöÑ„Å´„Åì„Åì„ÇíÊõ∏„ÅçÊèõ„Åà„Çã„Åì„Å®„Åå„É°„Ç§„É≥„Å´„Å™„Çã„ÄÇ\n",
    "prompt = \"Tell me a story about a dragon.\"\n",
    "\n",
    "# 4. „Åì„Åì„Åß„Éó„É≠„É≥„Éó„Éà„ÇíLLM„Å´ÂÖ•Âäõ„Åó„Å¶„ÄÅÂá∫Âäõ„ÇíÂæó„Çã„ÄÇ\n",
    "output = pipe(prompt, max_length=100, do_sample=True) # Â§âÊõ¥ÁÆáÊâÄ\n",
    "for item in output:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "218b4768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cea49dab64b4980866d309f67ff46e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \"Tell me a story about a dragon.\\n\\nDragon's are legendary creatures from all cultures around the world. They are often depicted as powerful, fire-breathing beasts with wings and a long tail. But what if I told you that not all dragons are created equal? What if there was a dragon that was different from all the others?\\n\\nMeet the gentle dragon, named Scorch. Scorch was born in a\"}\n",
      "{'generated_text': 'Tell me a story about a dragon.\\n\\nDragon stories are a classic tale of good vs. evil, where the dragon represents the powerful and feared creature that must be defeated by a brave hero. Here is a story about a dragon that you might enjoy:\\n\\nOnce upon a time, in a far-off land, there was a magnificent dragon named Scorch. Scorch was the largest and most powerful dragon in all the land, with scales'}\n",
      "{'generated_text': 'Tell me a story about a dragon. üêâ\\n\\nSure! Here is a story about a dragon:\\n\\nOnce upon a time, in a far-off land, there lived a magnificent dragon named Scorch. Scorch was the largest and most powerful dragon in all the land, with scales as red as fire and wings that stretched wider than the sky. He lived in a great cave deep within a towering mountain, and he'}\n",
      "Self-BLEU for output 1: 18.04\n",
      "Self-BLEU for output 2: 18.08\n",
      "Self-BLEU for output 3: 16.86\n",
      "Average Self-BLEU: 17.66\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import sacrebleu # ÂÆöÈáèÁöÑ„Å´Â§öÊßòÊÄß„ÇíÊ∏¨„Çã„Åü„ÇÅ„ÄÅmetric„ÇíËøΩÂä†\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(0)  # ÈÅ©ÂΩì„Å™SEEDÂÄ§„ÇíÂÖ•Âäõ„Åó„Å¶Âõ∫ÂÆö\n",
    "\n",
    "# 1. „É¢„Éá„É´Âêç\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. „É¢„Éá„É´„ÅÆ‰ΩúÊàê„ÄÇÁ∞°Âçò„ÅÆ„Åü„ÇÅ„Å´pipeline„Å®„ÅÑ„ÅÜ„ÉÑ„Éº„É´„ÇíÁî®„ÅÑ„Å¶„ÅÑ„Çã„ÄÇ\n",
    "# 2.1 torch_dtype=torch.float16„ÅØ16bit„ÅßË™≠„ÅøËæº„Åø„ÄÇÂæåËø∞„ÄÇ\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# 3. „Éó„É≠„É≥„Éó„ÉàÔºàÂÖ•ÂäõÊñáÔºâÔºöÂü∫Êú¨ÁöÑ„Å´„Åì„Åì„ÇíÊõ∏„ÅçÊèõ„Åà„Çã„Åì„Å®„Åå„É°„Ç§„É≥„Å´„Å™„Çã„ÄÇ\n",
    "prompt = \"Tell me a story about a dragon.\"\n",
    "\n",
    "# 4. „Åì„Åì„Åß„Éó„É≠„É≥„Éó„Éà„ÇíLLM„Å´ÂÖ•Âäõ„Åó„Å¶„ÄÅÂá∫Âäõ„ÇíÂæó„Çã„ÄÇ\n",
    "outputs = pipe(prompt, max_length=100, do_sample=True, num_return_sequences=3,\n",
    "              top_p=0.9, temperature=0.8) # ËøΩË®òÁÆáÊâÄ\n",
    "for item in outputs:\n",
    "    print(item)\n",
    "\n",
    "# (Option) Self-BLEU„ÅÆË®àÁÆó„ÄÇ‰Ωé„Åë„Çå„Å∞‰Ωé„ÅÑ„Åª„ÅÜ„Åå„ÅÑ„ÅÑ„ÄÇ\n",
    "scores = []\n",
    "generated_texts = [output[\"generated_text\"].strip() for output in outputs]\n",
    "for i, hypo in enumerate(generated_texts):\n",
    "    refs = generated_texts[:i] + generated_texts[i+1:] # ‰ªñ„ÅÆÂÖ®„Å¶„ÇíÂèÇÁÖß„Å®„Åô„Çã\n",
    "    bleu = sacrebleu.corpus_bleu([hypo], [refs]) # sacrebleu„ÅØ„É™„Çπ„Éà„ÇíÂèó„ÅëÂèñ„Çã\n",
    "    scores.append(bleu.score)\n",
    "\n",
    "# Self-BLEU„ÅÆÁµêÊûú„ÄÇ‰Ωé„Åë„Çå„Å∞‰Ωé„ÅÑ„Åª„ÅÜ„Åå„ÄÅÂ§öÊßòÊÄß„ÅåÈ´ò„ÅÑ„ÅÆ„ÅßÊ≥®ÊÑè„ÄÇ\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Self-BLEU for output {i+1}: {score:.2f}\")\n",
    "print(f\"Average Self-BLEU: {sum(scores)/len(scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "316ea2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(prompt, max_length=100, do_sample=True, num_return_sequences=3, top_p=1.0, top_k=0, temperature=1.0) # top_k=0„ÅØË®≠ÂÆö„Åó„Å™„ÅÑ„Å®„ÅÑ„ÅÜÊÑèÂë≥„ÄÇ„Å§„Åæ„ÇäÂÖ®„Å¶„ÅÆÂçòË™û„ÇíÂØæË±°„Å®„Åô„Çã\n",
    "pipe(prompt, max_length=100, do_sample=True, num_return_sequences=3, epsilon_cutoff=0.05, temperature=1.0) # epsilon_cutoff„Åß‰∏ãÈôê„ÇíÊåáÂÆö„ÄÇ0.02„Åå„Çà„ÅèÁî®„ÅÑ„Çâ„Çå„Å¶„ÅÑ„Çã„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dff23d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 0.19524600205379708)\n",
      "('a', 0.1682579039658947)\n",
      "('dragon', 0.13889383948672074)\n",
      "('flew', 0.10599583750891042)\n",
      "('over', 0.09134444373127171)\n",
      "('and', 0.08356431113117868)\n",
      "('castle', 0.07540317694785204)\n",
      "('into', 0.05754339372398352)\n",
      "('flames', 0.0475010249327733)\n",
      "('darkness', 0.03625006651761769)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "q = [(\"the\", 0.25), (\"a\", 0.20), (\"dragon\", 0.15), (\"flew\", 0.10), (\"over\", 0.08), \n",
    "     (\"and\", 0.07), (\"castle\", 0.06), (\"into\", 0.04), (\"flames\", 0.03), (\"darkness\", 0.02)]\n",
    "T = 1.5 # ‰∏ÄÊó¶1.5„Å´Ë®≠ÂÆö„ÄÇ\n",
    "\n",
    "exp_q = [0]*len(q) # ‰∏ÄÊó¶ÂàÜÂ≠ê„ÇíÂÖ®ÈÉ®Ë®àÁÆó„Åô„Çã„ÄÇ\n",
    "p = [None]*len(q) # p„ÇíÊ†ºÁ¥ç„Åô„Çã„É™„Çπ„Éà„Çí„ÅÇ„Çâ„Åã„Åò„ÇÅ‰Ωú„Å£„Å¶„Åä„Åè\n",
    "\n",
    "# „Åì„Åì„ÅßÂàÜÂ≠ê„Çí‰∏ÄÊ∞ó„Å´Ë®àÁÆó\n",
    "for i in range(len(q)):\n",
    "  exp_q[i] = math.exp(math.log(q[i][1]) / T) # [1]„Å™„ÅÆ„ÅØ„Çø„Éó„É´„Å†„Åã„Çâ„ÄÅÁ¢∫Áéá„ÅÆ„ÅøÂèñÂæó„Åó„Åü„ÅÑ\n",
    "\n",
    "# exp_q„Çísum„ÅßÂâ≤„Å£„Å¶„ÄÅp„ÇíÊ±Ç„ÇÅ„Çã„ÄÇ\n",
    "for i in range(len(q)):\n",
    "  p[i] = (q[i][0], exp_q[i] /sum(exp_q))\n",
    "  print(p[i])\n",
    "\n",
    "# [('the', 0.1952), ('a', 0.1683), ('dragon', 0.1389), ('flew', 0.1060), ('over', 0.0913),\n",
    "#  ('and', 0.0836), ('castle', 0.0754), ('into', 0.0575), ('flames', 0.0475), ('darkness', 0.0363)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993dfc8",
   "metadata": {},
   "source": [
    "# 1.2„É°„É¢„É™„ÇíÁØÄÁ¥Ñ„Åô„ÇãÔºöÈáèÂ≠êÂåñÔºàquantizationÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c884a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_CACHE_DIR\"] = \"/mnt/dx2_data/s2530074/.triton\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f2f3326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d85df7d5aac4e25a5ff7d8203f7f62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a story about a dragon.\n",
      "\n",
      "Sure! Here is a story about a dragon:\n",
      "\n",
      "Once upon a time, in a far-off land, there lived a magnificent dragon named Scorch. Scorch was the largest and most powerful dragon in all the land, with scales as black as coal and eyes that glowed like embers. He lived in a great, sprawling cave system deep within a mountain range, surrounded by a ho\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 1. „É¢„Éá„É´Âêç\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. „É¢„Éá„É´„ÅÆ‰ΩúÊàê„ÄÇquantization_config„ÅåÊñ∞„Åü„Å´ËøΩÂä†„Åï„Çå„Å¶„ÅÑ„Çã„ÄÇload_in_8bit=True„Å®„Åô„Çå„Å∞„Çà„ÅÑ„ÄÇ\n",
    "qconfig = BitsAndBytesConfig(load_in_8bit=True) # load_in_4bit=True„Å™„Çâ4bit\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=qconfig)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 3. „Éó„É≠„É≥„Éó„ÉàÔºàÂÖ•ÂäõÊñáÔºâÔºöÂü∫Êú¨ÁöÑ„Å´„Åì„Åì„ÇíÊõ∏„ÅçÊèõ„Åà„Çã„Åì„Å®„Åå„É°„Ç§„É≥„Å´„Å™„Çã„ÄÇ\n",
    "prompt = \"Tell me a story about a dragon.\"\n",
    "\n",
    "# 4. „Åì„Åì„Åß„Éó„É≠„É≥„Éó„Éà„ÇíLLM„Å´ÂÖ•Âäõ„Åó„Å¶„ÄÅÂá∫Âäõ„ÇíÂæó„Çã„ÄÇ\n",
    "output = pipe(prompt, max_length=100, do_sample=False)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "789c4b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73980b1a23847b3840051e7f6454e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  12860 MiB |  13015 MiB | 124557 MiB | 111697 MiB |\n",
      "|       from large pool |  12860 MiB |  13011 MiB | 102117 MiB |  89257 MiB |\n",
      "|       from small pool |      0 MiB |     68 MiB |  22439 MiB |  22439 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  12860 MiB |  13015 MiB | 124557 MiB | 111697 MiB |\n",
      "|       from large pool |  12860 MiB |  13011 MiB | 102117 MiB |  89257 MiB |\n",
      "|       from small pool |      0 MiB |     68 MiB |  22439 MiB |  22439 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  12860 MiB |  13014 MiB | 123906 MiB | 111046 MiB |\n",
      "|       from large pool |  12860 MiB |  13010 MiB | 101565 MiB |  88705 MiB |\n",
      "|       from small pool |      0 MiB |     68 MiB |  22341 MiB |  22340 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  12942 MiB |  13106 MiB |  59230 MiB |  46288 MiB |\n",
      "|       from large pool |  12872 MiB |  13032 MiB |  58868 MiB |  45996 MiB |\n",
      "|       from small pool |     70 MiB |     74 MiB |    362 MiB |    292 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  13687 KiB |   6322 MiB |  74247 MiB |  74233 MiB |\n",
      "|       from large pool |  12160 KiB |   6318 MiB |  49568 MiB |  49556 MiB |\n",
      "|       from small pool |   1527 KiB |     40 MiB |  24679 MiB |  24677 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     293    |     738    |     821 K  |     821 K  |\n",
      "|       from large pool |     227    |     292    |       9 K  |       8 K  |\n",
      "|       from small pool |      66    |     511    |     812 K  |     812 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     293    |     738    |     821 K  |     821 K  |\n",
      "|       from large pool |     227    |     292    |       9 K  |       8 K  |\n",
      "|       from small pool |      66    |     511    |     812 K  |     812 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     262    |     272    |    1116    |     854    |\n",
      "|       from large pool |     227    |     235    |     935    |     708    |\n",
      "|       from small pool |      35    |      37    |     181    |     146    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       2    |      92    |  416131    |  416129    |\n",
      "|       from large pool |       1    |      39    |    1504    |    1503    |\n",
      "|       from small pool |       1    |      64    |  414627    |  414626    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. „É¢„Éá„É´Âêç\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. „É¢„Éá„É´„ÅÆ‰ΩúÊàê„ÄÇÁ∞°Âçò„ÅÆ„Åü„ÇÅ„Å´pipeline„Å®„ÅÑ„ÅÜ„ÉÑ„Éº„É´„ÇíÁî®„ÅÑ„Å¶„ÅÑ„Çã„ÄÇ      \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 3. „Éó„É≠„É≥„Éó„ÉàÔºàÂÖ•ÂäõÊñáÔºâÔºöÂü∫Êú¨ÁöÑ„Å´„Åì„Åì„ÇíÊõ∏„ÅçÊèõ„Åà„Çã„Åì„Å®„Åå„É°„Ç§„É≥„Å´„Å™„Çã„ÄÇ\n",
    "prompt = \"Tell me a story about a dragon.\"\n",
    "\n",
    "# 4. „Åì„Åì„Åß„Éó„É≠„É≥„Éó„Éà„ÇíLLM„Å´ÂÖ•Âäõ„Åó„Å¶„ÄÅÂá∫Âäõ„ÇíÂæó„Çã„ÄÇ\n",
    "output = pipe(prompt, max_length=100, do_sample=False)\n",
    "# print(output[0][\"generated_text\"])\n",
    "\n",
    "print(torch.cuda.memory_summary()) # „Åì„Åì„Åß„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÁ¢∫Ë™ç„Åô„Çã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f9ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9338c433c54d8b8d2354346988998f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3695 MiB |  13015 MiB | 257989 MiB | 254294 MiB |\n",
      "|       from large pool |   3596 MiB |  13011 MiB | 208232 MiB | 204636 MiB |\n",
      "|       from small pool |     98 MiB |    150 MiB |  49757 MiB |  49658 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3695 MiB |  13015 MiB | 257989 MiB | 254294 MiB |\n",
      "|       from large pool |   3596 MiB |  13011 MiB | 208232 MiB | 204636 MiB |\n",
      "|       from small pool |     98 MiB |    150 MiB |  49757 MiB |  49658 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3694 MiB |  13014 MiB | 257324 MiB | 253629 MiB |\n",
      "|       from large pool |   3596 MiB |  13010 MiB | 207679 MiB | 204083 MiB |\n",
      "|       from small pool |     98 MiB |    150 MiB |  49645 MiB |  49546 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   6614 MiB |  13106 MiB |  65824 MiB |  59210 MiB |\n",
      "|       from large pool |   6442 MiB |  13032 MiB |  65290 MiB |  58848 MiB |\n",
      "|       from small pool |    172 MiB |    172 MiB |    534 MiB |    362 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   2850 MiB |   6322 MiB | 204227 MiB | 201377 MiB |\n",
      "|       from large pool |   2845 MiB |   6318 MiB | 151847 MiB | 149001 MiB |\n",
      "|       from small pool |      5 MiB |     44 MiB |  52380 MiB |  52375 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1414    |    1637    |    1034 K  |    1033 K  |\n",
      "|       from large pool |     227    |     292    |      27 K  |      27 K  |\n",
      "|       from small pool |    1187    |    1410    |    1007 K  |    1005 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1414    |    1637    |    1034 K  |    1033 K  |\n",
      "|       from large pool |     227    |     292    |      27 K  |      27 K  |\n",
      "|       from small pool |    1187    |    1410    |    1007 K  |    1005 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      88    |     272    |    1203    |    1115    |\n",
      "|       from large pool |       2    |     235    |     936    |     934    |\n",
      "|       from small pool |      86    |      86    |     267    |     181    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     107    |     172    |  525949    |  525842    |\n",
      "|       from large pool |      65    |      65    |    1832    |    1767    |\n",
      "|       from small pool |      42    |     107    |  524117    |  524075    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÁèæÂú®„ÅÆ„Çª„É´„Åæ„Åü„ÅØÂâç„ÅÆ„Çª„É´„Åß„Ç≥„Éº„Éâ„ÇíÂÆüË°å‰∏≠„Å´„ÄÅ„Ç´„Éº„Éç„É´ (Kernel) „Åå„ÇØ„É©„ÉÉ„Ç∑„É•„Åó„Åæ„Åó„Åü„ÄÇ\n",
      "\u001b[1;31m„Ç®„É©„Éº„ÅÆÂéüÂõ†„ÇíÁâπÂÆö„Åô„Çã„Å´„ÅØ„ÄÅ„Çª„É´ÂÜÖ„ÅÆ„Ç≥„Éº„Éâ„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "\u001b[1;31mË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØ<a href='https://aka.ms/vscodeJupyterKernelCrash'>„Åì„Å°„Çâ</a>„Çí„ÇØ„É™„ÉÉ„ÇØ„Åó„Åæ„Åô„ÄÇ\n",
      "\u001b[1;31mË©≥Á¥∞„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅJupyter <a href='command:jupyter.viewOutput'>„É≠„Ç∞</a> „ÇíÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 1. „É¢„Éá„É´Âêç\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. „É¢„Éá„É´„ÅÆ‰ΩúÊàê„ÄÇÁ∞°Âçò„ÅÆ„Åü„ÇÅ„Å´pipeline„Å®„ÅÑ„ÅÜ„ÉÑ„Éº„É´„ÇíÁî®„ÅÑ„Å¶„ÅÑ„Çã„ÄÇ      \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 3. „Éó„É≠„É≥„Éó„ÉàÔºàÂÖ•ÂäõÊñáÔºâÔºöÂü∫Êú¨ÁöÑ„Å´„Åì„Åì„ÇíÊõ∏„ÅçÊèõ„Åà„Çã„Åì„Å®„Åå„É°„Ç§„É≥„Å´„Å™„Çã„ÄÇ\n",
    "prompt = \"Tell me a story about a dragon.\"\n",
    "\n",
    "# 4. „Åì„Åì„Åß„Éó„É≠„É≥„Éó„Éà„ÇíLLM„Å´ÂÖ•Âäõ„Åó„Å¶„ÄÅÂá∫Âäõ„ÇíÂæó„Çã„ÄÇ\n",
    "output = pipe(prompt, max_length=100, do_sample=False)\n",
    "# print(output[0][\"generated_text\"])\n",
    "\n",
    "print(torch.cuda.memory_summary()) # „Åì„Åì„Åß„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÁ¢∫Ë™ç„Åô„Çã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
