{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151719c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '6853a6b313beb4e81e2cd4ca', 'name': 'Yuk34', 'fullname': 'Yuki Suzumura', 'email': 's2530074@gl.cc.uec.ac.jp', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/4a3223d290ed43e4f823c4a73ede09aa.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'dx2', 'role': 'write', 'createdAt': '2025-06-19T05:58:04.286Z'}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login(token=\"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\")  # Hugging Face のアクセストークンをここに\n",
    "\n",
    "info = whoami()\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca142f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/dx2_data/s2530074/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/assets\"\n",
    "\n",
    "print(os.environ.get(\"HF_HOME\"))\n",
    "print(os.environ.get(\"HF_HUB_CACHE\"))\n",
    "print(os.environ.get(\"HF_ASSETS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1358bb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 20 21:31:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:4F:00.0 Off |                  Off |\n",
      "| 41%   34C    P8             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:52:00.0 Off |                  Off |\n",
      "| 41%   31C    P0             22W /  140W |     181MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  |   00000000:53:00.0 Off |                  Off |\n",
      "| 41%   27C    P8             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000               On  |   00000000:56:00.0 Off |                  Off |\n",
      "| 41%   28C    P8             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A4000               On  |   00000000:57:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A4000               On  |   00000000:CE:00.0 Off |                  Off |\n",
      "| 41%   28C    P8             15W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A4000               On  |   00000000:D1:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             18W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A4000               On  |   00000000:D2:00.0 Off |                  Off |\n",
      "| 41%   27C    P8             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   8  NVIDIA RTX A4000               On  |   00000000:D5:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   9  NVIDIA RTX A4000               On  |   00000000:D6:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A          496930      C   ...ata/s2530074/.venv/bin/python        158MiB |\n",
      "|    2   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    3   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    4   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    5   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    6   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    7   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    8   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    9   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.set_device(1)  # GPU番号3を指定\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a021c6",
   "metadata": {},
   "source": [
    "# 2.Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d02a4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480a0f5523df41e7b173784c70c6cccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99efc2dd056c4b4bbc00145cad8be84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5601ab3da43d43678b3f08908c41dac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c5d57838004d5082161eb89ea90092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bb2b80e6c447d6843606f8da1f148d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e5b40e79e3490b847aaffdd28b2462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1778b9eebb71498b86a3851ac847ca47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4024e523287f4e1fac0063c898fb3008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f243d05e3344bb952304523386b294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7805a99d124d1a9b0806b937cfa932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996643011e8b49b0a9debb1362b06cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Please translate to Japanese: Tell me a story about a dragon.\\nTell me a story about a dragon.\\nPlease translate to Japanese: Can you tell me how to say this in Japanese?\\nCan you tell me how to say this in Japanese?\\nPlease translate to Japanese: Tell me the way to the station.\\nPlease translate to Japanese: Tell me about the trip.\\nPlease translate to Japanese: Tell me what happened.\\nPlease translate to Japanese: Tell me'}]\n",
      "[{'generated_text': 'Please translate to Japanese: \\nEnglish: Tell me a story about a dragon.\\nJapanese: 竜に話してください。\\n\\n### Context\\n\\n- 竜に話してください。\\n- 話してください。\\n\\n### English\\n\\n- Tell me a story about a dragon.\\n\\n### Japanese\\n\\n- 竜に話'}]\n",
      "[{'generated_text': \"Please write a short sentence: Tell me a story about a dragon.\\nI'm afraid I can't help you.\\nMy name is Aaron, I'm a writer and a software developer.\\nI've written many short stories, but I've never been able to write a novel.\\nI have a problem. I have too many ideas.\\nI have a bunch of ideas for stories, but I'm having trouble organizing them into a novel\"}]\n",
      "[{'generated_text': 'Please paraphrase this sentence: Tell me a story about a dragon.\\nIn other words, tell me a story about a dragon and don’t quote any text.\\nI’m not trying to be rude, but I’m curious to know how you’d write it.\\nI’ll even take a picture of it!\\nThe best way to tell a story about a dragon is to use your imagination.\\nTell me a story about'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# 1. モデル名\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\" # ここを変更した\n",
    "\n",
    "# 2. モデルの作成。簡単のためにpipelineというツールを用いている。\n",
    "# 2.1 torch_dtype=torch.float16は16bitで読み込み。後述。\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# 3. プロンプト（入力文）。ここをいま複数指定して、効果を試している。\n",
    "content = 'Tell me a story about a dragon.'\n",
    "prompts = [f\"Please translate to Japanese: {content}\",\n",
    "           f\"Please translate to Japanese: \\nEnglish: {content}\\nJapanese:\",\n",
    "           f\"Please write a short sentence: {content}\",\n",
    "           f\"Please paraphrase this sentence: {content}\",\n",
    "          ]\n",
    "\n",
    "# 4. ここでプロンプトをLLMに入力して、出力を得る。\n",
    "outputs = pipe(prompts, max_length=100, do_sample=True)\n",
    "# print(output[0][\"generated_text\"])\n",
    "for item in outputs:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae3c2fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9517d5824e5346b696d812295773bce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please translate to Japanese: \\nEnglish: Tell me a story about a dragon.\\nJapanese:', 'Please translate to Japanese: \\nEnglish: Today is a beautifulday.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to Japanese: \\nEnglish: Tell me a story about a dragon.\\nJapanese:', 'Please translate to Japanese: \\nEnglish: How are you?\\nJapanese: お元気ですか？\\n\\nPlease translate to Japanese: \\nEnglish: Today is a beautifulday.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to Japanese: \\nEnglish: Tell me a story about a dragon.\\nJapanese:']\n",
      "[{'generated_text': 'Please translate to Japanese: \\nEnglish: Tell me a story about a dragon.\\nJapanese: 竜に話してくれ。\\n\\nEnglish: Tell me a story about a turtle.\\nJapanese: 亀に話してくれ。\\n\\nEnglish: Tell me a story about a cat.\\nJapanese: 猫に話してくれ。\\n\\nEnglish: Tell me a story about a bird.\\nJapanese: 鳥に話してくれ。\\n\\nEnglish: Tell me a story about a fish.\\nJapanese: 魚に話してくれ。\\n\\nEnglish'}]\n",
      "[{'generated_text': 'Please translate to Japanese: \\nEnglish: Today is a beautifulday.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to Japanese: \\nEnglish: Tell me a story about a dragon.\\nJapanese: ドラゴンの物語をお話ししてください。\\n\\nPlease translate to Japanese: \\nEnglish: Today is a beautiful day.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to Japanese: \\nEnglish: The weather is beautiful today.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to'}]\n",
      "[{'generated_text': 'Please translate to Japanese: \\nEnglish: How are you?\\nJapanese: お元気ですか？\\n\\nPlease translate to Japanese: \\nEnglish: Today is a beautifulday.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to Japanese: \\nEnglish: Tell me a story about a dragon.\\nJapanese: ドラゴンの話を聞いてください。\\n\\nPlease translate to Japanese: \\nEnglish: What is your favorite color?\\nJapanese: 好きな色は？\\n\\nPlease translate to Japanese: \\nEnglish: I like the color green.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# 1. モデル名\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\" # ここを変更した\n",
    "\n",
    "# 2. モデルの作成。簡単のためにpipelineというツールを用いている。\n",
    "# 2.1 torch_dtype=torch.float16は16bitで読み込み。後述。\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# 3. プロンプト（入力文）。ここをいま複数指定して、効果を試している。\n",
    "content = 'Tell me a story about a dragon.'\n",
    "prompts = [f\"Please translate to Japanese: \\nEnglish: {content}\\nJapanese:\",\n",
    "           f\"Please translate to Japanese: \\nEnglish: Today is a beautifulday.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to Japanese: \\nEnglish: {content}\\nJapanese:\",\n",
    "           f\"Please translate to Japanese: \\nEnglish: How are you?\\nJapanese: お元気ですか？\\n\\nPlease translate to Japanese: \\nEnglish: Today is a beautifulday.\\nJapanese: 今日はいい天気ですね。\\n\\nPlease translate to Japanese: \\nEnglish: {content}\\nJapanese:\",\n",
    "          ]\n",
    "print(prompts)\n",
    "\n",
    "# 4. ここでプロンプトをLLMに入力して、出力を得る。\n",
    "outputs = pipe(prompts, max_length=150, do_sample=True) # 入力文が長いので、max_lengthを150に少し拡張。\n",
    "# print(output[0][\"generated_text\"])\n",
    "for item in outputs:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e443b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a480391915c40df81672075f0cc43a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant.\n",
      "<</SYS>>\n",
      "\n",
      "Please translate to Japanese: Tell me a story about a dragon. [/INST]\n",
      "---\n",
      "{'generated_text': '<s>[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\nPlease translate to Japanese: Tell me a story about a dragon. [/INST]  Of course! Here is a story about a dragon in Japanese:\\n\\n「一匹のドラゴンがいた。彼は高くて強大な竜のようで、炎を吐'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# 1. モデル名\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. モデルの作成。簡単のためにpipelineというツールを用いている。\n",
    "# 2.1 torch_dtype=torch.float16は16bitで読み込み。後述。\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please translate to Japanese: Tell me a story about a dragon.\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"（ここにLLMの応答を必要に応じて書く）\"},\n",
    "    # {\"role\": \"user\", \"content\": \"（user/assistantは交互に）\"},\n",
    "    # {\"role\": \"assistant\", \"content\": \"（会話履歴をこのように入力する）\"},\n",
    "    # {\"role\": \"user\", \"content\": \"（few-shotも同様の形式で入れてあげると良い）\"},\n",
    "    # ...\n",
    " ]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)\n",
    "print('---')\n",
    "\n",
    "outputs = pipe(prompt, max_length=100, do_sample=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808d42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393f60f2e69544aba161c01af298a8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant.\n",
      "<</SYS>>\n",
      "\n",
      "Please translate to Japanese: \n",
      "English: How are you? [/INST] Japanese: お元気ですか？ </s><s>[INST] Please translate to Japanese: \n",
      "English: Today is a beautifulday. [/INST] Japanese: 今日はいい天気ですね。 </s><s>[INST] Please translate to Japanese: \n",
      "English: Tell me a story about a dragon. [/INST]\n",
      "---\n",
      "[{'generated_text': '<s>[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\nPlease translate to Japanese: \\nEnglish: How are you? [/INST] Japanese: お元気ですか？ </s><s>[INST] Please translate to Japanese: \\nEnglish: Today is a beautifulday. [/INST] Japanese: 今日はいい天気ですね。 </s><s>[INST] Please translate to Japanese: \\nEnglish: Tell me a story about a dragon. [/INST] Japanese: ドラゴンの物語を聞いてみましょう。'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# 1. モデル名\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. モデルの作成。簡単のためにpipelineというツールを用いている。\n",
    "# 2.1 torch_dtype=torch.float16は16bitで読み込み。後述。\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please translate to Japanese: \\nEnglish: How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Japanese: お元気ですか？\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please translate to Japanese: \\nEnglish: Today is a beautifulday.\\n\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Japanese: 今日はいい天気ですね。\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please translate to Japanese: \\nEnglish: Tell me a story about a dragon.\"},\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)\n",
    "print('---')\n",
    "\n",
    "outputs = pipe(prompt, max_length=200, do_sample=True)\n",
    "print(outputs)\n",
    "# [{'generated_text': '<s>[INST] <<SYS>>\\nYou are a helpful assistant.\\n<</SYS>>\\n\\nPlease translate to Japanese: \\nEnglish: How are you? [/INST] Japanese: お元気ですか？ </s><s>[INST] Please translate to Japanese: \\nEnglish: Today is a beautifulday. [/INST] Japanese: 今日はいい天気ですね。 </s><s>[INST] Please translate to Japanese: \\nEnglish: Tell me a story about a dragon. [/INST] Japanese: ドラゴンの物語を聞いてみましょう。'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2e29832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 20 20:20:31 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:4F:00.0 Off |                  Off |\n",
      "| 41%   29C    P0             28W /  140W |     263MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:52:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             15W /  140W |     181MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  |   00000000:53:00.0 Off |                  Off |\n",
      "| 41%   27C    P8             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000               On  |   00000000:56:00.0 Off |                  Off |\n",
      "| 41%   28C    P8             19W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A4000               On  |   00000000:57:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A4000               On  |   00000000:CE:00.0 Off |                  Off |\n",
      "| 41%   27C    P5             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A4000               On  |   00000000:D1:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A4000               On  |   00000000:D2:00.0 Off |                  Off |\n",
      "| 41%   27C    P5             15W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   8  NVIDIA RTX A4000               On  |   00000000:D5:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   9  NVIDIA RTX A4000               On  |   00000000:D6:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    0   N/A  N/A          493486      C   ...ata/s2530074/.venv/bin/python        240MiB |\n",
      "|    1   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A          493486      C   ...ata/s2530074/.venv/bin/python        158MiB |\n",
      "|    2   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    3   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    4   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    5   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    6   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    7   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    8   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    9   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.set_device(1)  # GPU番号3を指定\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c7ac7",
   "metadata": {},
   "source": [
    "# 2.2Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da761e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225c599f47b74c9d85083f316ab40f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful Kansai-jin assistant.\n",
      "<</SYS>>\n",
      "\n",
      "Please translate to Japanese: Tell me a story about a dragon. [/INST]\n",
      "---\n",
      "[{'generated_text': '<s>[INST] <<SYS>>\\nYou are a helpful Kansai-jin assistant.\\n<</SYS>>\\n\\nPlease translate to Japanese: Tell me a story about a dragon. [/INST]  Oh, ho ho ho! *adjusts spectacles* A story about a dragon, you say? *crackles with excitement* Let me tell you a tale of a magnificent beast, straight from the heart of Kansai! *adjusts kimono*\\n\\nOnce upon a time, in the rolling hills of Kyoto, there lived a majestic dragon named Ryūjin. Ryūjin was no ordinary dragon, for he was said to possess the power of the gods themselves. His scales shone like the brightest jewels, and his wings stretched wide enough to block out the sun.\\n\\nNow, Ryūjin was a gentle creature, but he was also fierc'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# 1. モデル名                                                                                                                                                         \n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# 2. モデルの作成。簡単のためにpipelineというツールを用いている。                                                                                                     \n",
    "# 2.1 torch_dtype=torch.float16は16bitで読み込み。後述。                                                                                                              \n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Kansai-jin assistant.\"}, # ここに関西人という属性を記述する。\n",
    "    {\"role\": \"user\", \"content\": \"Please translate to Japanese: Tell me a story about a dragon.\"},\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)\n",
    "print('---')\n",
    "\n",
    "outputs = pipe(prompt, max_length=200, do_sample=True)\n",
    "print(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0043f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Oh, ho ho ho! *adjusts spectacles* A story about a dragon, you say? *crackles with excitement* Let me tell you a tale of a magnificent beast, straight from the heart of Kansai! *adjusts kimono*\n",
      "\n",
      "Once upon a time, in the rolling hills of Kyoto, there lived a majestic dragon named Ryūjin. Ryūjin was no ordinary dragon, for he was said to possess the power of the gods themselves. His scales shone like the brightest jewels, and his wings stretched wide enough to block out the sun.\n",
      "\n",
      "Now, Ryūjin was a gentle creature, but he was also fierc\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0]['generated_text'][len(prompt):]) # ここを追記。promptの長さで区切っている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00e108",
   "metadata": {},
   "source": [
    "# 2.3検索を用いた応答生成（RAG）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67f209e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snippet: Events [June 2, 2025] NAIST will hold a BIO JUKU on September 8-9. Events [June 2, 2025] Applications are now open for Long term internship. News [June 2, 2025] NAIST Edge BIO \"Generation of pluripotent stem cell-derived hearts in interspecies chimeric animals ..., title: NARA INSTITUTE of SCIENCE and TECHNOLOGY - NAIST, link: https://bsw3.naist.jp/eng/, snippet: 2024 NAIST Academic Encouragement Award (Division of Materials Science) Ceremony & Award Lecture will be held on March 8, 2025 ResearchAchievement . 2024-11-27. Ms. Chitlada Mani-Lata (D2) from the Laboratory for Organic Electronics Received the Company Award \"SCREEN Holdings Award\" at the 2024 Doc\"¦ ResearchAchievement ..., title: News - Naist, link: https://mswebs.naist.jp/en/, snippet: NAIST is an elite, research-intensive institution for students focused on graduate studies in science and technology.If your goal is to pursue high-level research or enter the tech/biotech industry or academia, NAIST is an excellent choice.It is not a general-purpose university but a specialized powerhouse in its niche.. he Nara Institute of Science and Technology (NAIST) is a legitimate and ..., title: is NAIST Good? - Rebellion Research, link: https://www.rebellionresearch.com/is-naist-good, snippet: 2023 NAIST Academic Award Encouragement Ceremony and Award Lecture will be held on February 24, 2024. EventReporting . 2023-12-05. The \"FY2023 Mid-Term Student Research Evaluation & Fellowship meeting\" was held at Division of Materials Science. ResearchAchievement . 2023-12-04 ..., title: Home | 奈良先端科学技術大学院大学 物質創成 ... - Naist, link: https://vm-mswebs.naist.jp/en/\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "print(search.invoke(\"NAIST\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b038601",
   "metadata": {},
   "source": [
    "## 課題：チャットシステム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b585f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fe94b6332b4f69a0762f395ab69175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Llama 2 Chat Start ===\n",
      "（'exit' または 'quit' で終了）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Of course! Here is a story about a dragon in Japanese:\n",
      "\n",
      "「一匹のドラゴンがいた。彼は高くて強大な竜のようで、炎を吐き、空を飛び、山を砕いた。」\n",
      "\n",
      "Translation:\n",
      "\n",
      "There was a dragon. He was tall and powerful, like a dragon, and he breathed fire, flew through the sky, and crushed mountains.\n",
      "\n",
      "Let me know if you would like me to translate more of the story!\n",
      "Assistant: Sure! Here is a \"Hello World\" program in Python:\n",
      "```\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "This will output \"Hello, World!\" when you run the program.\n",
      "\n",
      "Here's a breakdown of the code:\n",
      "\n",
      "* `print`: This is a function that prints something to the screen. In this case, it's printing the string \"Hello, World!\".\n",
      "* `(\"Hello, World!\"): This is a string, which is a type of data in Python that represents a sequence of characters.\n",
      "\n",
      "I hope this helps! Let me know if you have any questions.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# モデルの読み込み\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "pipe = pipeline(\"text-generation\", model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# チャット履歴の初期化\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# チャットループ\n",
    "print(\"=== Llama 2 Chat Start ===\")\n",
    "print(\"（'exit' または 'quit' で終了）\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    # ユーザーの発言を履歴に追加\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # プロンプトを生成\n",
    "    prompt = pipe.tokenizer.apply_chat_template(chat_history, tokenize=False)\n",
    "\n",
    "    # モデルに入力し、応答を生成\n",
    "    outputs = pipe(prompt, max_length=512, do_sample=True, temperature=0.7)\n",
    "    reply = outputs[0][\"generated_text\"].replace(prompt, \"\")  # プロンプト部分を除去\n",
    "\n",
    "    # 出力の後処理（チャンクが複数ある可能性があるため）\n",
    "    reply = reply.strip().split(\"</s>\")[0].strip()\n",
    "\n",
    "    # 応答を履歴に追加して表示\n",
    "    print(f\"Assistant: {reply}\")\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4d8380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 21 11:32:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:4F:00.0 Off |                  Off |\n",
      "| 41%   31C    P0             16W /  140W |     263MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:52:00.0 Off |                  Off |\n",
      "| 41%   32C    P0             33W /  140W |     181MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  |   00000000:53:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000               On  |   00000000:56:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             18W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A4000               On  |   00000000:57:00.0 Off |                  Off |\n",
      "| 41%   31C    P8             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A4000               On  |   00000000:CE:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A4000               On  |   00000000:D1:00.0 Off |                  Off |\n",
      "| 41%   31C    P5             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A4000               On  |   00000000:D2:00.0 Off |                  Off |\n",
      "| 41%   30C    P5             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   8  NVIDIA RTX A4000               On  |   00000000:D5:00.0 Off |                  Off |\n",
      "| 41%   31C    P5             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   9  NVIDIA RTX A4000               On  |   00000000:D6:00.0 Off |                  Off |\n",
      "| 41%   32C    P3             27W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    0   N/A  N/A          497260      C   ...ata/s2530074/.venv/bin/python        240MiB |\n",
      "|    1   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A          497260      C   ...ata/s2530074/.venv/bin/python        158MiB |\n",
      "|    2   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    3   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    4   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    5   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    6   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    7   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    8   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    9   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.set_device(1)  # GPU番号3を指定\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d8929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128dfaa6a43c4d16b84498f344279e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_497260/3757359511.py:13: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(elem_id=\"chatbox\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16)\n",
    "\n",
    "def respond(message, history):\n",
    "    prompt = f\"{message}\"\n",
    "    response = pipe(prompt, max_length=100, do_sample=False)[0][\"generated_text\"].replace(prompt, \"\")\n",
    "    return response\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbox\")\n",
    "    msg = gr.Textbox(label=\"Your message\")\n",
    "    send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    def user_send(user_message, history):\n",
    "        bot_message = respond(user_message, history)\n",
    "        history = history + [(user_message, bot_message)]\n",
    "        return history, \"\"\n",
    "\n",
    "    send_btn.click(user_send, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "\n",
    "    # CSSで高さを設定（必要に応じて）\n",
    "    demo.css = \"\"\"\n",
    "    #chatbox {\n",
    "        height: 500px;\n",
    "        overflow-y: scroll;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6213b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/dx2_data/s2530074/huggingface\n",
      "/mnt/dx2_data/s2530074/huggingface/hub\n",
      "/mnt/dx2_data/s2530074/huggingface/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/dx2_data/s2530074/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/assets\"\n",
    "\n",
    "print(os.environ.get(\"HF_HOME\"))\n",
    "print(os.environ.get(\"HF_HUB_CACHE\"))\n",
    "print(os.environ.get(\"HF_ASSETS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f3f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9a517be7ad410792773b6a709c3858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_496930/218865443.py:35: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(elem_id=\"chatbox\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 40, in user_send\n",
      "    bot_message = respond(user_message, history)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 28, in respond\n",
      "    outputs = pipe(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 302, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1431, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1438, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1338, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 400, in _forward\n",
      "    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2453, in generate\n",
      "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1617, in _validate_generated_length\n",
      "    raise ValueError(\n",
      "ValueError: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 40, in user_send\n",
      "    bot_message = respond(user_message, history)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 28, in respond\n",
      "    outputs = pipe(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 302, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1431, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1438, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1338, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 400, in _forward\n",
      "    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2453, in generate\n",
      "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1617, in _validate_generated_length\n",
      "    raise ValueError(\n",
      "ValueError: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 40, in user_send\n",
      "    bot_message = respond(user_message, history)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 28, in respond\n",
      "    outputs = pipe(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 302, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1431, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1438, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1338, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 400, in _forward\n",
      "    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2453, in generate\n",
      "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1617, in _validate_generated_length\n",
      "    raise ValueError(\n",
      "ValueError: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 40, in user_send\n",
      "    bot_message = respond(user_message, history)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 28, in respond\n",
      "    outputs = pipe(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 302, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1431, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1438, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1338, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 400, in _forward\n",
      "    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2453, in generate\n",
      "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1617, in _validate_generated_length\n",
      "    raise ValueError(\n",
      "ValueError: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 40, in user_send\n",
      "    bot_message = respond(user_message, history)\n",
      "  File \"/tmp/ipykernel_496930/218865443.py\", line 28, in respond\n",
      "    outputs = pipe(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 302, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1431, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1438, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1338, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 400, in _forward\n",
      "    output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2453, in generate\n",
      "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1617, in _validate_generated_length\n",
      "    raise ValueError(\n",
      "ValueError: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "pipe = pipeline(\"text-generation\",max_length=100, model=model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# systemメッセージは固定\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "\n",
    "def respond(user_message, history):\n",
    "    # historyは[(user_msg, bot_msg), ...] の形でGradioで保持されているので\n",
    "    # これを{\"role\":..., \"content\":...}形式に変換\n",
    "    chat_history = [system_message]\n",
    "    for user_msg, bot_msg in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "\n",
    "    # 新しいユーザー入力も追加\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    # トークナイザーのapply_chat_templateでprompt生成\n",
    "    prompt = pipe.tokenizer.apply_chat_template(chat_history, tokenize=False)\n",
    "\n",
    "    # モデルにpromptを渡して応答生成\n",
    "    outputs = pipe(prompt, max_length=100, do_sample=True, temperature=0.7)\n",
    "\n",
    "    reply = outputs[0][\"generated_text\"].replace(prompt, \"\").strip().split(\"</s>\")[0].strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbox\")\n",
    "    msg = gr.Textbox(label=\"Your message\")\n",
    "    send_btn = gr.Button(\"Send\")\n",
    "\n",
    "    def user_send(user_message, history):\n",
    "        bot_message = respond(user_message, history)\n",
    "        history = history + [(user_message, bot_message)]\n",
    "        return history, \"\"\n",
    "\n",
    "    send_btn.click(user_send, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
    "\n",
    "    demo.css = \"\"\"\n",
    "    #chatbox {\n",
    "        height: 500px;\n",
    "        overflow-y: scroll;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7862)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15966a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=\"alpindale/WizardLM-2-8x22B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a95df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfa81e827dc473c978083bf7bd5d8dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_509903/3867922056.py\", line 64, in on_submit\n",
      "    new_history, new_story = chat_flow(answer.strip(), story_history)\n",
      "NameError: name 'chat_flow' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/mnt/dx2_data/s2530074/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 904, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_509903/3867922056.py\", line 64, in on_submit\n",
      "    new_history, new_story = chat_flow(answer.strip(), story_history)\n",
      "NameError: name 'chat_flow' is not defined\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# LLMモデルの準備（お好みで変更してください）\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0  # GPU使うなら0に。CPUならこの引数外す。\n",
    ")\n",
    "\n",
    "# 初期プロンプト（物語の開始）\n",
    "initial_story = \"あなたは森の中で迷っています。前に進みますか？（はい／いいえ）\"\n",
    "\n",
    "def generate_next_story(current_story, answer):\n",
    "    \"\"\"\n",
    "    現在の物語の状況とユーザーの「はい」or「いいえ」に対して次の展開を生成する\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"以下は物語の現在の状況と選択肢です。\\n\"\n",
    "        f\"状況：{current_story}\\n\"\n",
    "        f\"ユーザーの選択：{answer}\\n\"\n",
    "        f\"この選択に応じて、次の物語の展開を100文字程度で書いてください。\"\n",
    "    )\n",
    "    output = pipe(prompt, max_length=300, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    # 生成テキストからプロンプト部分を除去\n",
    "    next_story = output.replace(prompt, \"\").strip()\n",
    "    # もし空文字や不自然な場合は適当にデフォルト返す\n",
    "    if not next_story or next_story.lower().startswith(\"はい\") or next_story.lower().startswith(\"いいえ\"):\n",
    "        next_story = \"物語は続きます。次の選択は「はい」か「いいえ」です。\"\n",
    "    return next_story\n",
    "\n",
    "def chat_flow(answer, story_history):\n",
    "    \"\"\"\n",
    "    ユーザーが「はい」「いいえ」を入力して物語を進める\n",
    "    story_historyは[最新の物語展開(文字列)]を1要素だけ保持しておく想定\n",
    "    \"\"\"\n",
    "    # 初回なら初期ストーリーを設定\n",
    "    if story_history is None or len(story_history) == 0:\n",
    "        current_story = initial_story\n",
    "        story_history = [initial_story]\n",
    "        return story_history, initial_story\n",
    "\n",
    "    current_story = story_history[-1]\n",
    "\n",
    "    # ユーザーの回答が「はい」か「いいえ」以外なら促す\n",
    "    if answer not in [\"はい\", \"いいえ\"]:\n",
    "        return story_history, \"「はい」か「いいえ」で答えてください。\"\n",
    "\n",
    "    next_story = generate_next_story(current_story, answer)\n",
    "    story_history.append(next_story)\n",
    "    return story_history, next_story\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## はい／いいえで進む物語\")\n",
    "    story_state = gr.State([])  # 物語履歴保存用\n",
    "\n",
    "    story_display = gr.Textbox(label=\"物語\", lines=10, interactive=False, value=initial_story)\n",
    "    answer_input = gr.Textbox(label=\"はい or いいえ で答えてください\", placeholder=\"はい または いいえ\")\n",
    "    submit_btn = gr.Button(\"進める\")\n",
    "\n",
    "    def on_submit(answer, story_history):\n",
    "        new_history, new_story = chat_flow(answer.strip(), story_history)\n",
    "        return new_history, new_story, \"\"\n",
    "\n",
    "    submit_btn.click(on_submit, inputs=[answer_input, story_state], outputs=[story_state, story_display, answer_input])\n",
    "\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7862)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e7c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/dx2_data/s2530074/huggingface\n",
      "/mnt/dx2_data/s2530074/huggingface/hub\n",
      "/mnt/dx2_data/s2530074/huggingface/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/dx2_data/s2530074/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/assets\"\n",
    "\n",
    "print(os.environ.get(\"HF_HOME\"))\n",
    "print(os.environ.get(\"HF_HUB_CACHE\"))\n",
    "print(os.environ.get(\"HF_ASSETS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb6ea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 21 11:39:11 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:4F:00.0 Off |                  Off |\n",
      "| 41%   33C    P0             22W /  140W |     263MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:52:00.0 Off |                  Off |\n",
      "| 41%   31C    P8             15W /  140W |     181MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  |   00000000:53:00.0 Off |                  Off |\n",
      "| 41%   28C    P8             15W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000               On  |   00000000:56:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             18W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A4000               On  |   00000000:57:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A4000               On  |   00000000:CE:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A4000               On  |   00000000:D1:00.0 Off |                  Off |\n",
      "| 41%   31C    P5             15W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A4000               On  |   00000000:D2:00.0 Off |                  Off |\n",
      "| 41%   29C    P5             13W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   8  NVIDIA RTX A4000               On  |   00000000:D5:00.0 Off |                  Off |\n",
      "| 41%   30C    P5             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   9  NVIDIA RTX A4000               On  |   00000000:D6:00.0 Off |                  Off |\n",
      "| 41%   32C    P3             19W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    0   N/A  N/A          509903      C   ...ata/s2530074/.venv/bin/python        240MiB |\n",
      "|    1   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A          509903      C   ...ata/s2530074/.venv/bin/python        158MiB |\n",
      "|    2   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    3   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    4   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    5   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    6   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    7   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    8   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    9   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.set_device(1)  # GPU番号3を指定\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
