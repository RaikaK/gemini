{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06678d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 21 14:19:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:4F:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A4000               On  |   00000000:52:00.0 Off |                  Off |\n",
      "| 41%   32C    P0             20W /  140W |     181MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A4000               On  |   00000000:53:00.0 Off |                  Off |\n",
      "| 41%   29C    P8             14W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A4000               On  |   00000000:56:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A4000               On  |   00000000:57:00.0 Off |                  Off |\n",
      "| 41%   31C    P8             18W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A4000               On  |   00000000:CE:00.0 Off |                  Off |\n",
      "| 41%   30C    P8             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A4000               On  |   00000000:D1:00.0 Off |                  Off |\n",
      "| 41%   31C    P5             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA RTX A4000               On  |   00000000:D2:00.0 Off |                  Off |\n",
      "| 41%   30C    P5             16W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   8  NVIDIA RTX A4000               On  |   00000000:D5:00.0 Off |                  Off |\n",
      "| 41%   30C    P5             15W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   9  NVIDIA RTX A4000               On  |   00000000:D6:00.0 Off |                  Off |\n",
      "| 41%   32C    P5             17W /  140W |      18MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    1   N/A  N/A          516481      C   ...ata/s2530074/.venv/bin/python        158MiB |\n",
      "|    2   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    3   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    4   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    5   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    6   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    7   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    8   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "|    9   N/A  N/A            3528      G   /usr/lib/xorg/Xorg                        4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.set_device(1)  # GPU番号3を指定\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e151fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/dx2_data/s2530074/huggingface\n",
      "/mnt/dx2_data/s2530074/huggingface/hub\n",
      "/mnt/dx2_data/s2530074/huggingface/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/mnt/dx2_data/s2530074/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/hub\"\n",
    "os.environ[\"HF_ASSETS_CACHE\"] = \"/mnt/dx2_data/s2530074/huggingface/assets\"\n",
    "\n",
    "print(os.environ.get(\"HF_HOME\"))\n",
    "print(os.environ.get(\"HF_HUB_CACHE\"))\n",
    "print(os.environ.get(\"HF_ASSETS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36f5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from bert_score import score\n",
    "\n",
    "# モデルの一覧\n",
    "models = {\n",
    "    \"WizardLM-2-8x22B\": \"alpindale/WizardLM-2-8x22B\",\n",
    "    \"LLaMA-2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "}\n",
    "\n",
    "# 検証用プロンプトと参照文（理想解）\n",
    "prompts = [\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "    \"What's the capital of Canada?\",\n",
    "    \"Write a Python function to compute Fibonacci numbers.\",\n",
    "    \"Summarize the main causes of World War II.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"The theory of relativity explains how time and space are linked for objects moving at a constant speed, especially for those moving close to the speed of light.\",\n",
    "    \"The capital of Canada is Ottawa.\",\n",
    "    \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\",\n",
    "    \"World War II was caused by the rise of fascist regimes, unresolved issues from World War I, and expansionist policies of Germany, Italy, and Japan.\"\n",
    "]\n",
    "\n",
    "# 応答を生成する関数\n",
    "def generate_response(model_id, tokenizer, model, prompt, max_new_tokens=200):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# メイン処理\n",
    "for model_name, model_id in models.items():\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    for prompt in prompts:\n",
    "        output = generate_response(model_id, tokenizer, model, prompt)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # BERTScore評価\n",
    "    print(f\"\\n--- Results for {model_name} ---\")\n",
    "    P, R, F1 = score(outputs, references, lang='en', verbose=False)\n",
    "\n",
    "    for i, (prompt, output, ref) in enumerate(zip(prompts, outputs, references)):\n",
    "        print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "        print(f\"Model Output:\\n{output.strip()}\")\n",
    "        print(f\"Reference:\\n{ref}\")\n",
    "        print(f\"BERTScore F1: {F1[i].item():.4f}\")\n",
    "\n",
    "    avg_f1 = F1.mean().item()\n",
    "    print(f\"\\n{model_name} Average BERTScore F1: {avg_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
